{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "MachineLearningProject",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ramj-j-UroUF"
      },
      "source": [
        "# **Automating the credit approval process using Machine Learning**\n",
        "## RSM316\n",
        "## Professor Kan\n",
        "## 07 April 2020\n",
        "##### Ramis Najam\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhjCNqATc04i"
      },
      "source": [
        "### Import the relevant packages "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGEc1yx-c04l",
        "outputId": "b133db9c-ab01-4534-fad1-302cfc6cbc7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import random\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import make_scorer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqzVXzdYc04s"
      },
      "source": [
        "### Read the original data and split it into two sets, 70% of the data is for training, 30% of the data is for validation (you can have a different split)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZxgMcmTc04t"
      },
      "source": [
        "warnings.filterwarnings('ignore')\n",
        "original_data = pd.read_csv('train_data.csv',index_col=0)\n",
        "# Split the original data into two subsets, eval_data is used for model comparison\n",
        "train_data, valid_data = train_test_split(original_data, test_size=0.3, random_state=0, shuffle=True)\n",
        "train_data.index = range(len(train_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rP_18sVc040"
      },
      "source": [
        "## Naive feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJwrVngLc042"
      },
      "source": [
        "train_data = pd.concat([train_data, valid_data], axis=0)\n",
        "train_data.index = range(len(train_data))\n",
        "train_data.loc[train_data['OCCUPATION']!=1,'OCCUPATION'] = 0\n",
        "cat_vars = ['MARRIAGE', 'EDUCATION']\n",
        "encoders = [OneHotEncoder(categories='auto') for _ in range(len(cat_vars))] \n",
        "encoded_tr = [encoders[i].fit_transform(train_data[[cat_var]]).todense() for i,cat_var in enumerate(cat_vars)]\n",
        "X = pd.concat([train_data.iloc[:,:-1].drop(cat_vars, axis=1), \n",
        "                     pd.DataFrame(np.concatenate(encoded_tr, axis=1))], axis=1)\n",
        "y = train_data.iloc[:,-1] \n",
        "X = X.rename(columns={0:'Marriage 1',1:'Marriage 2',2:'Marriage 3',3:'Edu 1',4:'Edu 2',5:'Edu 3',\n",
        "                                  6:'Edu 4',7:'Edu 5',8:'Edu 6',9:'Edu 7'})\n",
        "X = X.drop(['Marriage 3','Edu 7'], axis=1)\n",
        "\n",
        "X_valid = pd.DataFrame(X.iloc[-len(valid_data):])\n",
        "y_valid = train_data.iloc[-len(valid_data):,-1] \n",
        "X.drop(X.tail(len(valid_data)).index, inplace=True)\n",
        "y.drop(y.tail(len(valid_data)).index, inplace=True)\n",
        "\n",
        "for i in [0,1,2,3,4,5,8]:\n",
        "    X1 = X.iloc[:,i]\n",
        "    mean = X1.mean()\n",
        "    std = X1.std()\n",
        "    X.iloc[:,i] = (X.iloc[:,i]-mean)/std\n",
        "    X_valid.iloc[:,i] = (X_valid.iloc[:,i]-mean)/std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWLjoirIc046"
      },
      "source": [
        "### Create a custom score function for cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9ZCoyJ4c046"
      },
      "source": [
        "def custom_loss(ground_truth, predictions):\n",
        "    TN, FP, FN, TP = confusion_matrix(ground_truth,predictions,sample_weight=None).ravel()\n",
        "    TPR = TP/(TP+FN)\n",
        "    TNR = TN/(TN+FP)\n",
        "    return (TPR+TNR)/2\n",
        "\n",
        "my_custom_loss = make_scorer(custom_loss,greater_is_better=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5q4wCp5Dc04-"
      },
      "source": [
        "### Import a number of classifiers from sklearn.  <a href=\"https://xgboost.readthedocs.io/en/latest/index.html\" target=\"_blank\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBEU4wgCc05A"
      },
      "source": [
        "# Starter Code Imports\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "# Our Extra Imports\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "# Import for combining classifiers\n",
        "from sklearn.ensemble import VotingClassifier\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lC7hBvB9c05I"
      },
      "source": [
        "### Fitting various models using the data.   Here I use the default options for each classifier and fit the model using the training data and verify its performance on the validation data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfNnA0MQ7hN3",
        "outputId": "34d15ef1-4047-4a63-9509-12eb59f7f2d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\"\"\"\n",
        "These are the 16 models I test\n",
        "\"\"\"\n",
        "# List to hold the models \n",
        "default_models = []\n",
        "# 8 models given in starter code:\n",
        "default_models.append(('Default LDA',LinearDiscriminantAnalysis()))\n",
        "default_models.append(('Default LR',LogisticRegression()))\n",
        "default_models.append(('Default SVM',LinearSVC()))\n",
        "default_models.append(('Default KNN',KNeighborsClassifier()))\n",
        "default_models.append(('Default DecisionTree',DecisionTreeClassifier(random_state=0)))\n",
        "default_models.append(('Default RandomForest',RandomForestClassifier(random_state=0)))\n",
        "default_models.append(('Default GradientBoost',GradientBoostingClassifier(random_state=0)))\n",
        "default_models.append(('Default XGBoost',XGBClassifier(random_state=0)))\n",
        "# 8 new models I added:\n",
        "default_models.append(('Default Ridge',RidgeClassifier()))\n",
        "default_models.append(('Default BaggingClassifier',BaggingClassifier()))\n",
        "default_models.append(('Default Gaussian Naive Bayes',GaussianNB()))\n",
        "default_models.append(('Default QDA',QuadraticDiscriminantAnalysis()))\n",
        "default_models.append(('Default Multi-layerPerceptron',MLPClassifier()))\n",
        "default_models.append(('Default ExtraTreesClassifier',ExtraTreesClassifier()))\n",
        "default_models.append(('Default AdaBoostClassifier',AdaBoostClassifier()))\n",
        "default_models.append(('Default GaussianProcessClassifier',GaussianProcessClassifier()))\n",
        "\n",
        "# Test the performance of the default classifiers\n",
        "for classifier, model in default_models:\n",
        "    model.fit(X,y)\n",
        "    y_pred = model.predict(X)\n",
        "    y_valid_pred = model.predict(X_valid)\n",
        "    train_accuracy = accuracy_score(y,y_pred)\n",
        "    valid_accuracy = accuracy_score(y_valid,y_valid_pred)\n",
        "    TN, FP, FN, TP = confusion_matrix(y_valid,y_valid_pred,sample_weight=None).ravel()\n",
        "    TPR = TP/(TP+FN)\n",
        "    TNR = TN/(TN+FP)\n",
        "    print(\"Classifier: {}\".format(classifier))\n",
        "    ### I have commented out the accuracy data because Ifound it redundant - but I have kept it here for reference purposes\n",
        "    #print(\"Accuracy Score on Training Data = {:.4f}\".format(train_accuracy))    \n",
        "    #print(\"Performance on Validation Data:\")\n",
        "    #print(\"Accuracy Score = {:.4f}\".format(valid_accuracy))    \n",
        "    #print(\"True Positive Rate = {:.4f}\".format(TPR))\n",
        "    #print(\"True Negative Rate = {:.4f}\".format(TNR))\n",
        "    print(\"Average of True Positive and True Negative Rates = {:.4f}\".format( (TPR+TNR)/2 ))\n",
        "    print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classifier: Default LDA\n",
            "Average of True Positive and True Negative Rates = 0.8804\n",
            "\n",
            "\n",
            "Classifier: Default LR\n",
            "Average of True Positive and True Negative Rates = 0.8712\n",
            "\n",
            "\n",
            "Classifier: Default SVM\n",
            "Average of True Positive and True Negative Rates = 0.8712\n",
            "\n",
            "\n",
            "Classifier: Default KNN\n",
            "Average of True Positive and True Negative Rates = 0.8340\n",
            "\n",
            "\n",
            "Classifier: Default DecisionTree\n",
            "Average of True Positive and True Negative Rates = 0.7862\n",
            "\n",
            "\n",
            "Classifier: Default RandomForest\n",
            "Average of True Positive and True Negative Rates = 0.8526\n",
            "\n",
            "\n",
            "Classifier: Default GradientBoost\n",
            "Average of True Positive and True Negative Rates = 0.8827\n",
            "\n",
            "\n",
            "Classifier: Default XGBoost\n",
            "Average of True Positive and True Negative Rates = 0.8723\n",
            "\n",
            "\n",
            "Classifier: Default Ridge\n",
            "Average of True Positive and True Negative Rates = 0.8804\n",
            "\n",
            "\n",
            "Classifier: Default BaggingClassifier\n",
            "Average of True Positive and True Negative Rates = 0.8517\n",
            "\n",
            "\n",
            "Classifier: Default Gaussian Naive Bayes\n",
            "Average of True Positive and True Negative Rates = 0.8132\n",
            "\n",
            "\n",
            "Classifier: Default QDA\n",
            "Average of True Positive and True Negative Rates = 0.7681\n",
            "\n",
            "\n",
            "Classifier: Default Multi-layerPerceptron\n",
            "Average of True Positive and True Negative Rates = 0.8630\n",
            "\n",
            "\n",
            "Classifier: Default ExtraTreesClassifier\n",
            "Average of True Positive and True Negative Rates = 0.8392\n",
            "\n",
            "\n",
            "Classifier: Default AdaBoostClassifier\n",
            "Average of True Positive and True Negative Rates = 0.8546\n",
            "\n",
            "\n",
            "Classifier: Default GaussianProcessClassifier\n",
            "Average of True Positive and True Negative Rates = 0.8308\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y47VInyNjU00"
      },
      "source": [
        "### Here, I tune each of the classifiers. The parameters selected for tuning are shown for each Classifier. Next, Grid Search is used to find the best parameters using the options provided in the grids"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVgs7xuy2eXz",
        "outputId": "9436f4a1-485e-4ac6-ccbd-b23f1f27ee21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\"\"\"\n",
        "NOTE: This cell takes ~10 minutes to execute. This cell can be skipped during execution and the rest of the code will work as intended.\n",
        "Here, I tune all the models that can be tuned. If a classifier cannot be tuned, Imention it below.\n",
        "- 3 models saw no change after parameter tuning (LDA, LR, R)\n",
        "- 6 models saw improvements after parameter tuning (KNN, SVM, DT, XGB, QDA, ABC)\n",
        "- 4 models saw a worsening in performance after tuning (RF, GB, BC, ETC)\n",
        "- 2 models did not have tuning capabilities/could not be meaningfully tuned (GNB, GP)\n",
        "- 1 model was not tuned due to prohibitively long execution time (MLP)\n",
        "\n",
        "Grid Search was used to tune all the parameters for the classifiers. The parameters to tune\n",
        "were taken from the sklearn documentation. Unless otherwise stated, it served as the reference\n",
        "for all parameter tuning.\n",
        "https://scikit-learn.org/stable/supervised_learning.html#supervised-learning\n",
        "For XGBoost, I used:\n",
        "https://xgboost.readthedocs.io/en/latest/parameter.html\n",
        "Other online sources were browsed to compare which parameters were tuned.\n",
        "There was a significant trade-off between the number of parameters Ituned and the time it took\n",
        "For this reason, I believe that the reason why 4 models saw worse performance was due to inadequate parameter tuning provided.\n",
        "Since Grid Search takes very long to run with large inputs, Isaved the computationally intensive tuning\n",
        "for the classifiers that naturally performed better.\n",
        "\"\"\"\n",
        "\n",
        "# List to hold all the tuned models\n",
        "tuned_models = []\n",
        "\n",
        "#Tuned LDA\n",
        "solver = ['lsqr', 'svd', 'eigen'] # The different ways of solving the LDA\n",
        "shrinkage = ['auto', None] # Whether or not to use shrinkage in calculating LDA\n",
        "grid = dict(solver=solver, shrinkage=shrinkage)\n",
        "tuned_models.append(('Tuned LDA',LinearDiscriminantAnalysis(),grid))\n",
        "\n",
        "# Tuned LR\n",
        "penalty = ['l1', 'l2','none'] # Penalty methods in fitting the LR\n",
        "C = [0.1, 1, 10, 100] # C-values used to calculate the LR\n",
        "solver = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'] # The solving algorithm used\n",
        "grid = dict(penalty=penalty, C=C, solver=solver)\n",
        "tuned_models.append(('Tuned LR',LogisticRegression(),grid))\n",
        "    \n",
        "# Tuned SVM\n",
        "C = [100, 50, 10, 5, 1.0, 0.1, 0.01] # C-values used to calculate the SVM\n",
        "gamma = ['scale'] # Defines influence per training example\n",
        "kernel = ['linear', 'poly', 'rbf', 'sigmoid'] # The method used to calculate decision function\n",
        "grid = dict(kernel=kernel, C=C, gamma=gamma)\n",
        "tuned_models.append(('Tuned SVM',SVC(),grid)) ### I generalzie it to SVC, which includes the Linear included in the started code as well as poly, rbf, sigmoid\n",
        "\n",
        "# Tuned KNN\n",
        "n_neighbors = range(1,21) # The number of neighbours it uses in the queries\n",
        "weights = ['uniform', 'distance'] # How the weights for each value are calculated\n",
        "metric = ['euclidean', 'manhattan', 'minkowski'] # How distance is measured\n",
        "grid = dict(n_neighbors=n_neighbors,weights=weights,metric=metric)\n",
        "tuned_models.append(('Tuned KNN',KNeighborsClassifier(),grid))\n",
        "\n",
        "# Tuned DecisionTree\n",
        "criterion= ['gini', 'entropy'] # Measure of impurity calculation\n",
        "max_depth = [4,6,8,12] # Number of nodes a tree may have, to prevent overfitting\n",
        "grid = dict(criterion=criterion, max_depth=max_depth)\n",
        "tuned_models.append(('Tuned DecisionTree',DecisionTreeClassifier(), grid))\n",
        "\n",
        "# Tuned RandomForest\n",
        "n_estimators = [10, 100] # Number of trees in the forest\n",
        "max_features = ['sqrt', 'log2'] # Number of features to consider when deciding to split\n",
        "grid = dict(n_estimators=n_estimators, max_features=max_features)\n",
        "tuned_models.append(('Tuned RandomForest',RandomForestClassifier(), grid))\n",
        "\n",
        "# Tuned GradientBoost\n",
        "n_estimators = [10, 100,] # Number of boosting stages to perform\n",
        "learning_rate = [0.001, 0.01, 0.1] # Shrink factor for the contribution per tree\n",
        "subsample = [0.5, 0.7, 1.0] # Number of subsamples to be used for fitting\n",
        "max_depth = [3, 7, 9] # max number of nodes per tree\n",
        "grid = dict(learning_rate=learning_rate, n_estimators=n_estimators, subsample=subsample, max_depth=max_depth)\n",
        "tuned_models.append(('Tuned GradientBoost',GradientBoostingClassifier(), grid))\n",
        "\n",
        "# Tuned XGBoost\n",
        "n_estimators = [10, 100] # Number of boosting stages to perform\n",
        "learning_rate = [0.001, 0.01, 0.1] # Shrink factor for the contribution per tree\n",
        "subsample = [0.5, 0.7, 1.0] # Number of subsamples to be used for fitting\n",
        "max_depth = [3, 7, 9] # max number of nodes per tree\n",
        "grid = dict(learning_rate=learning_rate, n_estimators=n_estimators, subsample=subsample, max_depth=max_depth)\n",
        "tuned_models.append(('Tuned XGBoost',XGBClassifier(), grid))\n",
        "\n",
        "# Tuned Ridge\n",
        "alpha = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] # Regularization strength\n",
        "grid = dict(alpha=alpha)\n",
        "tuned_models.append(('Tuned Ridge',RidgeClassifier(), grid))\n",
        "\n",
        "# Tuned BaggingClassifier\n",
        "n_estimators = [10, 100] # Number of base estimators\n",
        "grid = dict(n_estimators=n_estimators)\n",
        "tuned_models.append(('Tuned BaggingClassifier',BaggingClassifier(), grid))\n",
        "\n",
        "# Tuned GaussianNaiveBayes\n",
        "# There is no parameter tuning for Bayesian Classifiers! Hence no tuned Bayesian Classifier\n",
        "\n",
        "# Tuned QDA\n",
        "reg_param = [0.1, 0.2, 0.3, 0.4, 0.5] # Regularization strength\n",
        "grid = dict(reg_param=reg_param)\n",
        "tuned_models.append(('Tuned QDA',QuadraticDiscriminantAnalysis(), grid))\n",
        "\n",
        "# Tuned Multi-layerPerceptron\n",
        "# NOTE: tuning this neural network was prohibitively time-consuming (many hours were required to execute);\n",
        "# Ihave included the parameters here to demonstrate the code that I intended to use;\n",
        "activation = ['identity', 'logistic', 'tanh', 'relu'] # Activation function for the hidden layer\n",
        "solver = ['lbfgs', 'sgd', 'adam'] # Solver for weight optimization\n",
        "alpha = [0.0001, 0.05] # L2 penalty (regularization parameter)\n",
        "learning_rate = ['constant', 'adaptive', 'invscaling'] # Marginal learning rate for weight updates\n",
        "grid = dict(activation=activation, solver=solver, alpha=alpha, learning_rate=learning_rate)\n",
        "# tuned_models.append(('Tuned MLP',MLPClassifier(), grid))\n",
        "\n",
        "# Tuned ExtraTreesClassifier\n",
        "n_estimators = [1, 10, 100] # Number of trees in the forest\n",
        "criterion = ['gini', 'entropy'] # Measure of impurity calculation\n",
        "grid = dict(n_estimators=n_estimators, criterion=criterion)\n",
        "tuned_models.append(('Tuned ExtraTreesClassifier',ExtraTreesClassifier(), grid))\n",
        "\n",
        "# Tuned AdaBoostClassifier\n",
        "algorithm = ['SAMME', 'SAMME.R'] # Algorithm used to calculate the boosting\n",
        "n_estimators = [1, 10, 50, 100] # Number of boosting stages to perform\n",
        "grid = dict(n_estimators=n_estimators, algorithm=algorithm)\n",
        "tuned_models.append(('Tuned AdaBoostClassifier',AdaBoostClassifier(), grid))\n",
        "\n",
        "# Tuned GaussianProcessClassifier\n",
        "# The parameters provided do not allow for meaningful parameter tuning for this classifier\n",
        "\n",
        "print(\"Tuned Algorithms!\")\n",
        "# Tuned algorithms\n",
        "for classifier, model, grid in tuned_models:\n",
        "    # The following two lines of code are the same for all Grid Search parameter tuning\n",
        "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "    grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring=my_custom_loss, error_score=0)\n",
        "\n",
        "    #### Everything below this is the same\n",
        "    model = grid_search.fit(X, y)\n",
        "    y_pred = model.predict(X)\n",
        "    y_valid_pred = model.predict(X_valid)\n",
        "    train_accuracy = accuracy_score(y,y_pred)\n",
        "    valid_accuracy = accuracy_score(y_valid,y_valid_pred)\n",
        "    TN, FP, FN, TP = confusion_matrix(y_valid,y_valid_pred,sample_weight=None).ravel()\n",
        "    TPR = TP/(TP+FN)\n",
        "    TNR = TN/(TN+FP)\n",
        "    print(\"Classifier: {}\".format(classifier))\n",
        "    ### Ihave commented out the accuracy data because I found it redundant - but Ihave kept it here for reference purposes\n",
        "    #print(\"Accuracy Score on Training Data = {:.4f}\".format(train_accuracy))    \n",
        "    #print(\"Performance on Validation Data:\")\n",
        "    #print(\"Accuracy Score = {:.4f}\".format(valid_accuracy))    \n",
        "    #print(\"True Positive Rate = {:.4f}\".format(TPR))\n",
        "    #print(\"True Negative Rate = {:.4f}\".format(TNR))\n",
        "    print(\"Average of True Positive and True Negative Rates = {:.4f}\".format( (TPR+TNR)/2 ))\n",
        "\n",
        "    ### Code added in to print the best parameter values\n",
        "    print(\"Best output using %s\" % (model.best_params_))\n",
        "    print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tuned Algorithms!\n",
            "Classifier: Tuned LDA\n",
            "Average of True Positive and True Negative Rates = 0.8804\n",
            "Best output using {'shrinkage': 'auto', 'solver': 'lsqr'}\n",
            "\n",
            "\n",
            "Classifier: Tuned LR\n",
            "Average of True Positive and True Negative Rates = 0.8712\n",
            "Best output using {'C': 1, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
            "\n",
            "\n",
            "Classifier: Tuned SVM\n",
            "Average of True Positive and True Negative Rates = 0.8804\n",
            "Best output using {'C': 0.1, 'gamma': 'scale', 'kernel': 'linear'}\n",
            "\n",
            "\n",
            "Classifier: Tuned KNN\n",
            "Average of True Positive and True Negative Rates = 0.8175\n",
            "Best output using {'metric': 'manhattan', 'n_neighbors': 13, 'weights': 'distance'}\n",
            "\n",
            "\n",
            "Classifier: Tuned DecisionTree\n",
            "Average of True Positive and True Negative Rates = 0.8496\n",
            "Best output using {'criterion': 'entropy', 'max_depth': 4}\n",
            "\n",
            "\n",
            "Classifier: Tuned RandomForest\n",
            "Average of True Positive and True Negative Rates = 0.8630\n",
            "Best output using {'max_features': 'sqrt', 'n_estimators': 100}\n",
            "\n",
            "\n",
            "Classifier: Tuned GradientBoost\n",
            "Average of True Positive and True Negative Rates = 0.8453\n",
            "Best output using {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.7}\n",
            "\n",
            "\n",
            "Classifier: Tuned XGBoost\n",
            "Average of True Positive and True Negative Rates = 0.8952\n",
            "Best output using {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.5}\n",
            "\n",
            "\n",
            "Classifier: Tuned Ridge\n",
            "Average of True Positive and True Negative Rates = 0.8804\n",
            "Best output using {'alpha': 0.1}\n",
            "\n",
            "\n",
            "Classifier: Tuned BaggingClassifier\n",
            "Average of True Positive and True Negative Rates = 0.8775\n",
            "Best output using {'n_estimators': 100}\n",
            "\n",
            "\n",
            "Classifier: Tuned QDA\n",
            "Average of True Positive and True Negative Rates = 0.8175\n",
            "Best output using {'reg_param': 0.1}\n",
            "\n",
            "\n",
            "Classifier: Tuned ExtraTreesClassifier\n",
            "Average of True Positive and True Negative Rates = 0.8340\n",
            "Best output using {'criterion': 'gini', 'n_estimators': 100}\n",
            "\n",
            "\n",
            "Classifier: Tuned AdaBoostClassifier\n",
            "Average of True Positive and True Negative Rates = 0.8619\n",
            "Best output using {'algorithm': 'SAMME', 'n_estimators': 10}\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lm-hLB4Dc05N"
      },
      "source": [
        "# In this step, I find the optimal threshold for each of the default and tuned classifiers. As a Bonus, I also use the VotingClassifier class to combine the classifiers. Grid Search is then used to find the optimal weighting to assign to this combination classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEEbyJS5c05X",
        "outputId": "471dc36d-2e6a-448a-b176-ca66e600ee4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\"\"\"\n",
        "Here, I tune the threshold for all the classifiers\n",
        "This includes the (1) Default, (2) Tuned, and the (3) Combination classifiers\n",
        "\"\"\"\n",
        "# Create list to hold data for threshold output\n",
        "check_thresholds = []\n",
        "\n",
        "# Defaults classifiers\n",
        "dlda = LinearDiscriminantAnalysis()\n",
        "check_thresholds.append(dlda)\n",
        "dlr = LogisticRegression()\n",
        "check_thresholds.append(dlr)\n",
        "dsvm = LinearSVC()\n",
        "# check_thresholds.append(dsvm) ####### can't be used in the loop since it cannot give probabilities\n",
        "dknn = KNeighborsClassifier()\n",
        "check_thresholds.append(dknn)\n",
        "ddt = DecisionTreeClassifier(random_state=0)\n",
        "check_thresholds.append(ddt)\n",
        "drf = RandomForestClassifier(random_state=0)\n",
        "check_thresholds.append(drf)\n",
        "dgb = GradientBoostingClassifier(random_state=0)\n",
        "check_thresholds.append(dgb)\n",
        "dxgb = XGBClassifier(random_state=0)\n",
        "check_thresholds.append(dxgb)\n",
        "dr = RidgeClassifier() \n",
        "# check_thresholds.append(dr) ######## can't be used in the loop since it cannot give probabilities\n",
        "dbg = BaggingClassifier()\n",
        "check_thresholds.append(dbg)\n",
        "dgnb = GaussianNB()\n",
        "check_thresholds.append(dgnb)\n",
        "dqda = QuadraticDiscriminantAnalysis()\n",
        "check_thresholds.append(dqda)\n",
        "dmlp = MLPClassifier()\n",
        "check_thresholds.append(dmlp)\n",
        "detc = ExtraTreesClassifier()\n",
        "check_thresholds.append(detc)\n",
        "dabc = AdaBoostClassifier()\n",
        "check_thresholds.append(dabc)\n",
        "dgpc = GaussianProcessClassifier()\n",
        "check_thresholds.append(dgpc)\n",
        "\n",
        "# Tuned classifiers\n",
        "# The parameters used here are the best ones given from Grid Search in the above cell\n",
        "tlda = LinearDiscriminantAnalysis(shrinkage='auto', solver='lsqr')\n",
        "check_thresholds.append(tlda)\n",
        "tlr = LogisticRegression(C=1, penalty='l2', solver='newton-cg')\n",
        "check_thresholds.append(tlr)\n",
        "tsvm = SVC(C=0.1, gamma='scale', kernel='linear')\n",
        "# check_thresholds.append(tsvm) ############################### can't be used in the loop\n",
        "tknn = KNeighborsClassifier(metric='manhattan', n_neighbors=13, weights='distance')\n",
        "check_thresholds.append(tknn)\n",
        "tdt = DecisionTreeClassifier(criterion='entropy', max_depth=4)\n",
        "check_thresholds.append(tdt)\n",
        "trf = RandomForestClassifier(max_features='log2', n_estimators=100)\n",
        "check_thresholds.append(trf)\n",
        "tgb = GradientBoostingClassifier(learning_rate=0.1, max_depth=9, n_estimators=100, subsample=0.5) ############################ OUR BEST classifier\n",
        "check_thresholds.append(tgb)\n",
        "txgb = XGBClassifier(learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.5)\n",
        "check_thresholds.append(txgb)\n",
        "tr = RidgeClassifier(alpha=0.1)\n",
        "# check_thresholds.append(tr) ############################### can't be used in the loop\n",
        "tbg = BaggingClassifier(n_estimators=100)\n",
        "check_thresholds.append(tbg)\n",
        "tqda = QuadraticDiscriminantAnalysis(reg_param=0.1)\n",
        "check_thresholds.append(tqda)\n",
        "tetc = ExtraTreesClassifier(criterion='entropy', n_estimators=100)\n",
        "check_thresholds.append(tetc)\n",
        "tabc = AdaBoostClassifier(algorithm='SAMME', n_estimators=10)\n",
        "check_thresholds.append(tabc)\n",
        "\n",
        "\n",
        "# --------------------------------------------Combination classifier!-----------------------------------------------------------\n",
        "# This combination combines 14 classifiers (Ridge and SVM do not support the 'predict_proba' method hence they cannot be used)\n",
        "# It uses Grid Search to determine the optimal weight of each classifier to use\n",
        "# For each classifier, Iuse either the default or tuned version, depending on which performs better\n",
        "# Reference: https://scikit-learn.org/stable/modules/ensemble.html\n",
        "combo=VotingClassifier(estimators=[('TunedKNNeighbours', tknn), ('TunedLinearDiscrimantAnalysis', tlda), ('TunedLogisticRegression', tlr),\n",
        "                                  ('TunedDecisionTree', tdt), ('DefaultRandomForest', drf), ('DefaultGradientBoosting', dgb),\n",
        "                                  ('TunedXGBoost', txgb), ('DefaultBaggingClassifier', dbg), ('TunedQuadraticDiscriminantAnalysis', tqda),\n",
        "                                  ('DefaultExtraTrees', detc), ('TunedAdaBoost', tabc), ('DefaultGaussianNB', dgnb),\n",
        "                                  ('DefaultMultilayerPerceptron', dmlp), ('DefaultGaussianProcess', dgpc)], voting='soft')\n",
        "params = {} # Since the params are already specified in the classifiers, there is no need to define additional ones here\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "combo = GridSearchCV(estimator=combo, param_grid=params, cv=cv)\n",
        "check_thresholds.append(combo)\n",
        "\n",
        "# Fit all the classifiers\n",
        "for c in check_thresholds:\n",
        "  c.fit(X,y)\n",
        "\n",
        "# Find the optimal threshold for each classifier (prints the classifiers in order)\n",
        "for c in check_thresholds:\n",
        "  THRESHOLD = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "  results = pd.DataFrame(columns=[\"THRESHOLD\", \"accuracy\", \"true positive rate\", \"true negative rate\", \"(TPR+TNR)/2\"]) # df to store results\n",
        "  results['THRESHOLD'] = THRESHOLD                                                           # threshold column\n",
        "  Q = c.predict_proba(X_valid)[:,1]\n",
        "  running_max = [0,0] # threshold, running max\n",
        "  for i in range(9):                                                                         # iterate over each threshold    \n",
        "    preds = np.where(Q>THRESHOLD[i], 1, 0)                                                 # if prob > threshold, predict 1   \n",
        "    valid_accuracy = accuracy_score(y_valid,preds)\n",
        "    TN, FP, FN, TP = confusion_matrix(y_valid,preds,sample_weight=None).ravel()\n",
        "    TPR = TP/(TP+FN)\n",
        "    TNR = TN/(TN+FP)\n",
        "    results.iloc[i,1] = valid_accuracy \n",
        "    results.iloc[i,2] = TPR\n",
        "    results.iloc[i,3] = TNR                                                              \n",
        "    results.iloc[i,4] = (TPR+TNR)/2\n",
        "    if (TPR+TNR)/2 > running_max[1]: \n",
        "      running_max[1] = (TPR+TNR)/2 \n",
        "      running_max[0] = THRESHOLD[i]\n",
        "  results.style.hide_index()\n",
        "  #print(results.T.to_string(header=False)) # Don't need to see all the redundant data so I comment it out\n",
        "  print(str(c)[0:13] + \", Threshold: \" + str(running_max[0]) + \", max \" + str(running_max[1]))\n",
        "  print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LinearDiscrim, Threshold: 0.2, max 0.8804347826086957\n",
            "\n",
            "\n",
            "LogisticRegre, Threshold: 0.4, max 0.885643115942029\n",
            "\n",
            "\n",
            "KNeighborsCla, Threshold: 0.4, max 0.8340126811594203\n",
            "\n",
            "\n",
            "DecisionTreeC, Threshold: 0.1, max 0.786231884057971\n",
            "\n",
            "\n",
            "RandomForestC, Threshold: 0.3, max 0.870018115942029\n",
            "\n",
            "\n",
            "GradientBoost, Threshold: 0.5, max 0.8826992753623188\n",
            "\n",
            "\n",
            "XGBClassifier, Threshold: 0.6, max 0.880661231884058\n",
            "\n",
            "\n",
            "BaggingClassi, Threshold: 0.3, max 0.8731884057971014\n",
            "\n",
            "\n",
            "GaussianNB(pr, Threshold: 0.1, max 0.8534873188405797\n",
            "\n",
            "\n",
            "QuadraticDisc, Threshold: 0.3, max 0.7753623188405797\n",
            "\n",
            "\n",
            "MLPClassifier, Threshold: 0.6, max 0.8713768115942029\n",
            "\n",
            "\n",
            "ExtraTreesCla, Threshold: 0.3, max 0.8659420289855073\n",
            "\n",
            "\n",
            "AdaBoostClass, Threshold: 0.5, max 0.8546195652173914\n",
            "\n",
            "\n",
            "GaussianProce, Threshold: 0.5, max 0.8360507246376812\n",
            "\n",
            "\n",
            "LinearDiscrim, Threshold: 0.2, max 0.8804347826086957\n",
            "\n",
            "\n",
            "LogisticRegre, Threshold: 0.4, max 0.885643115942029\n",
            "\n",
            "\n",
            "KNeighborsCla, Threshold: 0.4, max 0.8525815217391304\n",
            "\n",
            "\n",
            "DecisionTreeC, Threshold: 0.1, max 0.870018115942029\n",
            "\n",
            "\n",
            "RandomForestC, Threshold: 0.3, max 0.8752264492753623\n",
            "\n",
            "\n",
            "GradientBoost, Threshold: 0.1, max 0.8804347826086957\n",
            "\n",
            "\n",
            "XGBClassifier, Threshold: 0.5, max 0.8951539855072463\n",
            "\n",
            "\n",
            "BaggingClassi, Threshold: 0.4, max 0.8691123188405797\n",
            "\n",
            "\n",
            "QuadraticDisc, Threshold: 0.2, max 0.8731884057971014\n",
            "\n",
            "\n",
            "ExtraTreesCla, Threshold: 0.3, max 0.8618659420289855\n",
            "\n",
            "\n",
            "AdaBoostClass, Threshold: 0.5, max 0.8618659420289855\n",
            "\n",
            "\n",
            "GridSearchCV(, Threshold: 0.2, max 0.8720561594202898\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLcci0Wuc05p"
      },
      "source": [
        "## I find that the XGBoost Classifier with learning_rate=0.01, max_depth=7, n_estimators=100, and subsample=0.5 provides the best (TPR+TNR)/2. Further, this occurs at an optimal threshold of 0.5.\n",
        "\n",
        "Interestingly, the XGBoost Classifier works better than even the combination classifier, which seems to suggest that the other classifiers are not as efficient and do not generate any predictive value in combination with the XGBoost Classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqcMjtL_c05p"
      },
      "source": [
        "train_data = pd.read_csv('train_data.csv',index_col=0)\n",
        "### In the real test, sample_test_data.csv will be replaced by test_data.csv\n",
        "eval_data = pd.read_csv('sample_test_data.csv',index_col=0)\n",
        "train_data = pd.concat([train_data, eval_data], axis=0)\n",
        "train_data.index = range(len(train_data))\n",
        "train_data.loc[train_data['OCCUPATION']!=1,'OCCUPATION'] = 0\n",
        "cat_vars = ['MARRIAGE', 'EDUCATION']\n",
        "encoders = [OneHotEncoder(categories='auto') for _ in range(len(cat_vars))] \n",
        "encoded_tr = [encoders[i].fit_transform(train_data[[cat_var]]).todense() for i,cat_var in enumerate(cat_vars)]\n",
        "X = pd.concat([train_data.iloc[:,:-1].drop(cat_vars, axis=1), pd.DataFrame(np.concatenate(encoded_tr, axis=1))], axis=1)\n",
        "y = train_data.iloc[:,-1] \n",
        "X = X.rename(columns={0:'Marriage 1',1:'Marriage 2',2:'Marriage 3',3:'Edu 1',4:'Edu 2',5:'Edu 3',\n",
        "                                  6:'Edu 4',7:'Edu 5',8:'Edu 6',9:'Edu 7'})\n",
        "X = X.drop(['Marriage 3','Edu 7'], axis=1)\n",
        "\n",
        "X_test = pd.DataFrame(X.iloc[-len(eval_data):])\n",
        "y_test = train_data.iloc[-len(eval_data):,-1] \n",
        "X.drop(X.tail(len(eval_data)).index, inplace=True)\n",
        "y.drop(y.tail(len(eval_data)).index, inplace=True)\n",
        "\n",
        "for i in [0,1,2,3,4,5,8]:\n",
        "    X1 = X.iloc[:,i]\n",
        "    mean = X1.mean()\n",
        "    std = X1.std()\n",
        "    X.iloc[:,i] = (X.iloc[:,i]-mean)/std\n",
        "    X_test.iloc[:,i] = (X_test.iloc[:,i]-mean)/std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47M_gpWwc05s",
        "outputId": "3f316f67-cae4-447e-90f2-98f0b0382b29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "### THIS IS THE CHOSEN CLASSIFIER\n",
        "### THRESHOLD OF 0.5 is used\n",
        "### txbg is the name of the chosen classifier\n",
        "THRESHOLD = 0.5\n",
        "txbg = XGBClassifier(n_estimators=100, learning_rate=0.1, subsample=0.5, max_depth=7)\n",
        "txbg.fit(X, y)\n",
        "Q = txbg.predict_proba(X_test)[:,1]\n",
        "y_test_pred = np.where(Q>THRESHOLD, 1, 0)\n",
        "print(y_test_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}